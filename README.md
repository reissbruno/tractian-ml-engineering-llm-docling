# Tractian ML Engineering - LLM Challenge

A **Multimodal RAG (Retrieval-Augmented Generation)** system for processing and querying technical PDF documents. Built for the Tractian Machine Learning Engineering challenge.

## ğŸš€ Features

- âœ… **PDF Upload & Processing**: Extract text, tables, images, and formulas from technical PDFs
- âœ… **Intelligent Chunking**: Adaptive chunking based on content type (formulas, tables, procedures, etc.)
- âœ… **OpenAI Embeddings**: text-embedding-3-large (3072 dimensions) via langchain-openai
- âœ… **Vector Search**: ChromaDB for efficient similarity search
- âœ… **LLM Integration**: Support for OpenAI GPT-4 and Google Gemini
- âœ… **User Management**: JWT authentication with SQLite database
- âœ… **Web Interface**: Complete frontend for document upload and Q&A

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI   â”‚ â—„â”€â”€ REST API
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â–º Document Processing Pipeline
       â”‚    â”œâ”€â”€ Docling (high quality) / FastPDF (fast)
       â”‚    â”œâ”€â”€ Image Extraction (PyMuPDF)
       â”‚    â”œâ”€â”€ Content Cleaning
       â”‚    â”œâ”€â”€ Adaptive Chunking
       â”‚    â””â”€â”€ OpenAI Embeddings (text-embedding-3-large)
       â”‚
       â”œâ”€â”€â–º Vector Store (ChromaDB)
       â”‚    â””â”€â”€ Per-user collections
       â”‚
       â”œâ”€â”€â–º Database (SQLite)
       â”‚    â”œâ”€â”€ Users & Auth
       â”‚    â”œâ”€â”€ Documents metadata
       â”‚    â””â”€â”€ Images (base64)
       â”‚
       â””â”€â”€â–º LLM Service
            â”œâ”€â”€ OpenAI GPT-4
            â””â”€â”€ Google Gemini
```

## ğŸ“‹ Requirements

- Python 3.10+
- 8GB+ RAM (for document processing)
- OpenAI API key or Google Gemini API key

## ğŸš€ Quick Start

### 1. Clone the repository

```bash
git clone https://github.com/reissbruno/tractian-ml-engineering-llm.git
cd tractian-ml-engineering-llm
```

### 2. Create virtual environment

```bash
python -m venv .venv

# Windows
.venv\Scripts\activate

# Linux/Mac
source .venv/bin/activate
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Set up environment variables

Create a `.env` file:

```env
# OpenAI (required if using OpenAI)
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_MODEL=gpt-4o

# Google Gemini (required if using Gemini)
GOOGLE_API_KEY=your-gemini-api-key-here
GEMINI_MODEL=gemini-1.5-pro

# JWT Secret (change in production!)
JWT_SECRET_KEY=your-secret-key-here
JWT_EXPIRE_MINUTES=30

# Optional: Logging
LOG_LEVEL=INFO
API_NAME=tractian-ml-engineering-llm
```

### 5. Run the server

```bash
uvicorn server:app --reload --host 0.0.0.0 --port 8000
```

Or use the Makefile:

```bash
make run
```

### 6. Access the application

- **Web Interface:** http://localhost:8000
- **API Documentation:** http://localhost:8000/docs
- **Alternative Docs:** http://localhost:8000/redoc

## ğŸ“– Usage Guide

### Step 1: Login or Register

When you first access the application at http://localhost:8000, you'll see the login screen:

![Login Screen](images/tela-login.png)

If you don't have an account yet, click on "Criar uma conta aqui" to register a new user.

### Step 2: Upload Documents

After logging in, you'll be redirected to the document management page. Here you can:
- Upload PDF files (single or multiple)
- Choose between two processing modes:
  - **Fast**: Quick processing using PyMuPDF (~2-3s per page)
  - **Docling**: High-quality extraction with better table/formula recognition (~10-15s per page)
- View all your uploaded documents with their processing status

![Document Management](images/detalhes-documentos.png)

If no documents have been uploaded yet, you'll see an empty state:

![Empty State](images/tela-sem-documento.png)

### Step 3: Configure LLM Provider

Before asking questions, you can choose your preferred LLM provider and model:
- **OpenAI**: GPT-4o, GPT-4-turbo, GPT-3.5-turbo
- **Google Gemini**: gemini-1.5-pro, gemini-1.5-flash

![LLM Selection](images/tela-escolha-llm.png)

### Step 4: Ask Questions

Once your documents are processed, navigate to the chat interface to ask questions:
- Type your question in natural language
- The system will search through your documents using semantic similarity
- Receive answers with references to the source pages
- Images from the relevant pages are automatically included in the context

![Query Response](images/tela-resposta.png)

The response includes:
- **Answer**: Generated by the LLM based on the retrieved context
- **References**: Links to specific pages in the source documents
- **Context**: Relevant text chunks and images extracted from your PDFs

## ğŸ”Œ API Endpoints

### Upload Documents

```bash
POST /documents
Content-Type: multipart/form-data

# Upload single file
curl -X POST "http://localhost:8000/documents" \
  -F "files=@document.pdf" \
  -F "processor=fast"

# Upload multiple files
curl -X POST "http://localhost:8000/documents" \
  -F "files=@doc1.pdf" \
  -F "files=@doc2.pdf" \
  -F "processor=docling"
```

**Response:**
```json
{
  "message": "Documents processed successfully",
  "documents_indexed": 2,
  "total_chunks": 128
}
```

**Parameters:**
- `processor`: `"fast"` (default, faster) or `"docling"` (higher quality)

### Ask Questions

```bash
POST /question
Content-Type: application/json

curl -X POST "http://localhost:8000/question" \
  -H "Content-Type: application/json" \
  -H "X-LLM-Provider: openai" \
  -H "X-LLM-Model: gpt-4o" \
  -d '{
    "question": "What is the power consumption of the motor?"
  }'
```

**Response:**
```json
{
  "answer": "The motor's power consumption is 2.3 kW at 60Hz line frequency.",
  "references": [
    "[Motor_Manual.pdf - Page 15]",
    "[Technical_Specs.pdf - Page 3]"
  ]
}
```

**Headers:**
- `X-LLM-Provider`: `openai` or `gemini` (optional, defaults to env var)
- `X-LLM-Model`: Model name (optional, defaults to env var)

### List Documents

```bash
GET /documents

curl -X GET "http://localhost:8000/documents"
```

### Delete Document

```bash
DELETE /documents/{doc_id}

curl -X DELETE "http://localhost:8000/documents/abc-123-def"
```

### User Authentication

```bash
# Register
POST /register
{
  "user_name": "john",
  "senha": "password123"
}

# Login
POST /login
{
  "user_name": "john",
  "senha": "password123"
}
```

## ğŸ³ Docker Deployment

```bash
# Build and run
docker-compose up --build

# Run in background
docker-compose up -d

# Stop
docker-compose down
```

Access at: http://localhost:8000

## ğŸ› ï¸ Development

### Project Structure

```
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ auth/               # Authentication & database
â”‚   â”‚   â”œâ”€â”€ auth.py         # JWT, password hashing
â”‚   â”‚   â””â”€â”€ database.py     # SQLAlchemy models
â”‚   â”œâ”€â”€ services/           # Core services
â”‚   â”‚   â”œâ”€â”€ document_processor.py          # Docling processing
â”‚   â”‚   â”œâ”€â”€ fast_pdf_processor.py          # Fast PDF processing
â”‚   â”‚   â”œâ”€â”€ document_processor_vectorized.py  # Batch processing
â”‚   â”‚   â”œâ”€â”€ chunking_service.py            # Text chunking
â”‚   â”‚   â”œâ”€â”€ embeddings_service.py          # CLIP embeddings
â”‚   â”‚   â”œâ”€â”€ vector_store_service.py        # ChromaDB
â”‚   â”‚   â”œâ”€â”€ llm_service.py                 # LLM integration
â”‚   â”‚   â”œâ”€â”€ content_cleaner.py             # Markdown cleaning
â”‚   â”‚   â””â”€â”€ document_analyzer.py           # Content analysis
â”‚   â”œâ”€â”€ models.py           # Pydantic models
â”‚   â””â”€â”€ logger.py           # Centralized logging
â”œâ”€â”€ static/                 # Frontend files
â”‚   â”œâ”€â”€ login.html
â”‚   â”œâ”€â”€ register.html
â”‚   â”œâ”€â”€ documents.html
â”‚   â””â”€â”€ chat.html
â”œâ”€â”€ server.py               # FastAPI application
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ Dockerfile              # Docker configuration
â”œâ”€â”€ docker-compose.yml      # Docker Compose setup
â”œâ”€â”€ Makefile                # Development commands
â””â”€â”€ README.md               # This file
```

### Run Tests

```bash
pytest

# With coverage
pytest --cov=src tests/

# Specific test file
pytest tests/test_chunking_service.py
```

### Code Quality

```bash
# Format code
make format

# Lint
make lint

# Type check
make typecheck
```

## ğŸ¯ How It Works

### 1. Document Processing Pipeline

```
PDF â†’ Docling/FastPDF â†’ Markdown + Images
                      â†“
              Content Cleaning
                      â†“
              Adaptive Chunking
                      â†“
         OpenAI Embeddings (text-embedding-3-large)
                      â†“
              ChromaDB Storage
```

**Processors:**
- **Docling**: High quality extraction (tables, formulas, figures) - slower
- **FastPDF**: Fast extraction with heuristics - faster

### 2. Adaptive Chunking

Content is analyzed and chunked differently based on type:

| Content Type | Chunk Size | Overlap | Use Case |
|--------------|------------|---------|----------|
| Tables       | 300        | 50      | Structured data |
| Formulas     | 1200       | 150     | Mathematical content |
| Warnings     | 250        | 20      | Safety info |
| Procedures   | 400        | 80      | Step-by-step |
| Narrative    | 800        | 100     | General text |

### 3. RAG Query Flow

```
User Question
     â†“
Query Embedding (OpenAI text-embedding-3-large)
     â†“
Vector Search (ChromaDB) â†’ Top 5 chunks
     â†“
Fetch Associated Images
     â†“
Build LLM Context (text + images)
     â†“
LLM Generation (GPT-4/Gemini)
     â†“
Answer + References
```

## ğŸ”§ Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `OPENAI_API_KEY` | - | OpenAI API key (required if using OpenAI) |
| `OPENAI_MODEL` | `gpt-4o` | Default OpenAI model |
| `GOOGLE_API_KEY` | - | Google Gemini API key (required if using Gemini) |
| `GEMINI_MODEL` | `gemini-1.5-pro` | Default Gemini model |
| `JWT_SECRET_KEY` | `dev-secret-key...` | JWT signing key |
| `JWT_EXPIRE_MINUTES` | `30` | Token expiration time |
| `LOG_LEVEL` | `INFO` | Logging level |
| `API_NAME` | `tractian-ml-engineering-llm` | Logger name |

**Note:** LLM provider and model can be specified per request using HTTP headers (`X-LLM-Provider` and `X-LLM-Model`) instead of environment variables. Defaults to OpenAI with `gpt-4o`.

## ğŸ“Š Performance

### Processing Speed (approximate)

| Document Type | Pages | Time | Memory |
|---------------|-------|------|--------|
| Simple PDF    | 10    | ~20s | 500MB  |
| Technical Doc | 50    | ~2min| 1GB    |

### Query Response Time

- **Vector Search**: ~100-200ms
- **LLM Generation**: 2-5s (depends on context size)
- **Total**: ~2-5s per question